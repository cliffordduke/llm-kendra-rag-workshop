{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20a7bde3-1df3-45d5-9536-0bf8c9d06819",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG with Kendra and Falcon 40b Instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01e701e6",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "### Kendra Index\n",
    "\n",
    "---\n",
    "\n",
    "We will leverage the previous labs Kendra Index for demonstration purposes, in order for this notebook to work, please add `AmazonKendraFullAccess` policy to the IAM role\n",
    "\n",
    "1. Navigate to your notebook instance `https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/notebook-instances/` and open the associated IAM Role\n",
    "![Notebook instance](./assets/lab2/01_notebook_instance.png)\n",
    "\n",
    "2. Attach the `AmazonKendraFullAccess` policy to the role\n",
    "![Notebook instance](./assets/lab2/02_permissions.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Lab: Adding RAG capabilities to Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dccc0c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We start by setting up our variables for the workshop, add your Kendra Index ID found in the Kendra Console, we will also populate the current AWS region and the API Key you were provided for Falcon API Access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e4ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"api_url\": 'https://falcon.cliffordduke.cloud',\n",
    "    \"api_key\": '',\n",
    "    \"kendra_index\": '', \n",
    "    \"region\": 'us-west-2'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18168b79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's install and load the required dependencies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5849e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 langchain wikipedia --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52935919-1a41-41b7-8cfd-245f63d09b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import AmazonAPIGateway\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dd71d7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We start with defining a langchain supported llm, in this case we are using an already deployed `AmazonAPIGateway` that proxies to a cluster of `Falcon 40b Instruct` models hosted on Amazon SageMaker Hosted Inference.\n",
    "\n",
    "Next we will also configure the `AmazonKendraRetriever` allowing langchain to programmatically access our Kendra document index\n",
    "\n",
    "If you are interested in deploying your own API cluster, you can use this [CDK Template](https://github.com/cliffordduke/cdk-llm-api-gateway)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f71e0-8a3e-4bdf-a644-6c20fc54c312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = AmazonAPIGateway(\n",
    "    api_url=CONFIG['api_url'],\n",
    "    headers= {\n",
    "        \"X-API-Key\": CONFIG['api_key']\n",
    "    }\n",
    ")\n",
    "\n",
    "falcon_kwargs = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "llm.model_kwargs = falcon_kwargs\n",
    "\n",
    "retriever = AmazonKendraRetriever(\n",
    "    index_id=CONFIG['kendra_index'],\n",
    "    region_name=CONFIG['region']\n",
    ")\n",
    "\n",
    "\n",
    "def print_qa(result):\n",
    "    bold, unbold = \"\\033[1m\", \"\\033[0m\"\n",
    "    print(f'{bold}Answer{unbold}: {result[\"result\"]}\\n\\n{bold}Sources:{unbold}')\n",
    "    for doc in result['source_documents']:\n",
    "        print(f'''\\n{doc.metadata[\"title\"]}\\n{doc.metadata[\"source\"]}\\n{doc.metadata[\"excerpt\"]}\\n''')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac769c67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next let's try some basic prompting capabilties, we start by creating a prompt template to wrap around your user input using the `PromptTemplate` class,  We then load both the Prompt template and Kendra index into a RetrievalQA chain.\n",
    "\n",
    "For advanced users, feel free to try out different prompt templates and see how that can affect the overall generated completions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41709146-b1bf-4da6-8e2d-86836c794277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "The following is a friendly conversation between a human and an AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it\n",
    "does not know.\n",
    "{context}\n",
    "Instruction: Based on the above documents, provide a detailed answer for, {question} Answer \"don't know\" if not present in the document. Solution:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c5cbf3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have the chain set up, lets try some prompts! The kendra index was loaded with AWS Whitepapers, so let's ask it some AWS related questions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942fb89-0a97-4fef-b2b0-2fb3d9b6ba6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_qa(qa(\"What is SageMaker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c66430-a1ba-4aae-8136-8d851fcca8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_qa(qa(\"When should I choose single-region vs multi-region architecture\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65846ea5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Awesome! as you can see, the chain passes the user query to Kendra, which returns a list of documents it thinks is most relevant to the user question. We then use the Large Langage Model to help extract information from the documents to provide a more concise answer!\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cea10c9",
   "metadata": {},
   "source": [
    "### Adding Memory\n",
    "\n",
    "---\n",
    "\n",
    "It's great that we already have a working chain that can respond to queries, but each query is treated as a single question, what happens if a user asks a follow up question? For that, we need to give the chain some Short-Term Memory!\n",
    "\n",
    "In this example, lets use DynamoDB as a memory store to cache chat history, we first begin creating a DynamoDB table to store this information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba7301-4aa5-492f-b7ab-faa1851df800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dynamodb = boto3.resource('dynamodb', region_name=CONFIG['region'])\n",
    "\n",
    "try:\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=\"SessionTable\",\n",
    "        KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
    "        AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
    "        BillingMode=\"PAY_PER_REQUEST\",\n",
    "    )\n",
    "    table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n",
    "except dynamodb.meta.client.exceptions.ResourceInUseException:\n",
    "    print(\"DynamoDB Already Exists\")\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bd5f3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Once the table is ready, we will use the `DynamoDBChatMessageHistory` class to apply an external data store for the `ConversationBufferMemory`\n",
    "\n",
    "We can then create a `ConversationalRetriverChain` chain using our previously create llm \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fea11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qaHistory = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"2\")\n",
    "\n",
    "qaMemory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", input_key='question', output_key='answer', chat_memory=qaHistory, return_messages=True\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever, \n",
    "    memory=qaMemory,\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def question(input):\n",
    "    bold, unbold = \"\\033[1m\", \"\\033[0m\"\n",
    "    result = qa({\"question\": input})\n",
    "    print(f'{bold}Answer{unbold}: {result[\"answer\"]}\\n\\n{bold}Sources:{unbold}')\n",
    "    for doc in result['source_documents']:\n",
    "        print(f'''\\n{doc.metadata[\"title\"]}\\n{doc.metadata[\"source\"]}\\n{doc.metadata[\"excerpt\"]}\\n''')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936ef947",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's try asking it some information like before\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf082baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question(\"what is SageMaker\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978f8f9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "But this time, we try a follow up question that may be missing some context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c150f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question(\"What capabilities does it have?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b739b12e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We see that it has the ability to take previous conversations, and apply it as context to the follow up question!\n",
    "\n",
    "---\n",
    "\n",
    "### External Tools\n",
    "\n",
    "---\n",
    "\n",
    "We can apply this with not just your own data, but through external tools as well, for example using wikipedia to pull information. Remember though, each LLM has a context window limit that restricts how much information you can pass into the prompt!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=500\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wiki\",\n",
    "        description=\"useful for finding general information, use this\",\n",
    "        func=wikipedia.run,\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"When did AWS first launch?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391219f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
    "name": "conda_python3",
    "display_name": "conda_python3",
    "language": "python"
  },
  "language_info": {
    "name": "python",
    "version": "3.10.10",
    "mimetype": "text/x-python",
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "pygments_lexer": "ipython3",
    "nbconvert_exporter": "python",
    "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
